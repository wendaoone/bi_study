{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:03:19.390689Z",
     "start_time": "2020-11-09T12:03:14.808524Z"
    }
   },
   "source": [
    "# Thinking1:关联规则中的支持度、置信度和提升度代表的什么，如何计算\n",
    "\n",
    "    \n",
    "   |     | 概念  | 公式  |\n",
    "   |  ----  | ----  |  ----  |\n",
    "   | 支持度  | 商品在指定交易集合中出现的次数 |商品次数/总交易次数 |\n",
    "   | 置信度  | 在商品A出现的情况下，商品b出现的概率，也就是商品B在有商品A出现的交易中出现的次数 |  商品AB同时出现的次数/商品A出现的次数 |\n",
    "   | 提升度  | 商品A和商品B同时出现时候，商品B对商品A的支持程度 |支持度/置信度 |\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking2 关联规则与协同过滤的区别\n",
    "    \n",
    "     关联规则：是基于交易行为对物体固有的内部关系进行发掘，找出其频繁项集\n",
    "     协同过滤：是基于用户的历史行为习惯进行分析，挖掘用户喜好来进行推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking3 为什么我们需要多种推荐算法\n",
    "\n",
    "   各种推荐算法都有其优点以及适用场景，比如在电商系统中，对于一些冷启动的用户或这商品，可以通过关联规则方式进行前期推荐，等到有一定的用户行为或用户对商品的操作反馈信息后，可以使用协同过滤的方式推荐。通过关联规则可以发掘一些整体上的宏观规律性质的信息，协同过滤更具有个性化的需求\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking4 关联规则中的最小支持度、最小置信度该如何确定\n",
    " \n",
    " 通过经验数据来设置，比如交易数据量很大，此时每个商品在总商品中出现的次数都很低，如果设置的最小支持度或者最小置信度过大，则很可能筛选不出任何结果\n",
    " 也可以将每个商品的支持度，置信度计算出来，从大到小排序，然后卡定符合业务需求的位置所对应的指出度或置信度为最小支持度或者最小置信度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking5  都有哪些常见的回归分析方法，评价指标是什么\n",
    "\n",
    "  常见的回归分析方法有：一元线性回归，多元线性回归 非线性回归\n",
    "  评价指标是通过损失函数来评价的，有mse(更容易收敛)  MAE(可解释性更强) RMSE(既收敛又兼具可解释性)  R方值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action1  针对MarketBasket数据集进行购物篮分析（频繁项集及关联规则挖掘）[kaggle](https://www.kaggle.com/dragonheir/basket-optimisation)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T12:47:47.101344Z",
     "start_time": "2020-11-14T12:47:46.877205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      support                           itemsets\n",
      "0    0.020397                          (almonds)\n",
      "1    0.033329                          (avocado)\n",
      "2    0.033729                         (brownies)\n",
      "3    0.087188                          (burgers)\n",
      "4    0.030129                           (butter)\n",
      "..        ...                                ...\n",
      "98   0.020131  (mineral water, whole wheat rice)\n",
      "99   0.022930             (spaghetti, olive oil)\n",
      "100  0.025197              (spaghetti, pancakes)\n",
      "101  0.021197                (spaghetti, shrimp)\n",
      "102  0.020931              (tomatoes, spaghetti)\n",
      "\n",
      "[103 rows x 2 columns]\n",
      "                  from               to       sup      conf      lift\n",
      "0               (eggs)        (burgers)  0.028796  0.160237  1.837830\n",
      "1            (burgers)           (eggs)  0.028796  0.330275  1.837830\n",
      "2            (burgers)   (french fries)  0.021997  0.252294  1.476173\n",
      "3            (burgers)  (mineral water)  0.024397  0.279817  1.173883\n",
      "4            (burgers)      (spaghetti)  0.021464  0.246177  1.413918\n",
      "..                 ...              ...       ...       ...       ...\n",
      "66  (whole wheat rice)  (mineral water)  0.020131  0.343964  1.442993\n",
      "67         (olive oil)      (spaghetti)  0.022930  0.348178  1.999758\n",
      "68          (pancakes)      (spaghetti)  0.025197  0.265077  1.522468\n",
      "69            (shrimp)      (spaghetti)  0.021197  0.296642  1.703760\n",
      "70          (tomatoes)      (spaghetti)  0.020931  0.306043  1.757755\n",
      "\n",
      "[67 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "通过mlxtend的apriori方式对数据进行分析\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import fptools as fp\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "# 读取数据\n",
    "datas=pd.read_csv('data/Market_Basket_Optimisation.csv',header=None)\n",
    "# 通过迭代器方式对pd读取出来的数据转成list数组，并且出去其中的nan\n",
    "itemsets = [[y for y in x if pd.notna(y)] for x in datas.values.tolist()]\n",
    "# itemDits={}\n",
    "# for items in itemsets:\n",
    "#     for item in items:\n",
    "#         if item not in itemDits.keys():\n",
    "#             itemDits[item] = 1\n",
    "#         else:\n",
    "#             itemDits[item] = itemDits[item] + 1\n",
    "# itemDits = sorted(itemDits.items(), key=lambda item:item[1], reverse=True)\n",
    "\n",
    "# 转换为算法可接受模型（布尔值）one-hot操作\n",
    "te = TransactionEncoder()\n",
    "df_tf = te.fit_transform(itemsets)\n",
    "df = pd.DataFrame(df_tf, columns=te.columns_)\n",
    "\n",
    "# 设置支持度求频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True)\n",
    "# 求关联规则,设置最小置信度为0.15\n",
    "print(frequent_itemsets)\n",
    "rules = association_rules(frequent_itemsets,metric='confidence',  min_threshold=0.15)\n",
    "# 设置最小提升度\n",
    "rules = rules.drop(rules[rules.lift < 1.0].index)\n",
    "# # 设置标题索引并打印结果\n",
    "rules.rename(columns={'antecedents': 'from', 'consequents': 'to', 'support': 'sup', 'confidence': 'conf'}, inplace=True)\n",
    "rules = rules[['from', 'to', 'sup', 'conf', 'lift']]\n",
    "print(rules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
