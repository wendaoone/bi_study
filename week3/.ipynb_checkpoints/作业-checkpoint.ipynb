{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用用户标签来指导业务（如何提升业务）\n",
    "    可以使用用户标签实现精准营销，分析产品的潜在用户和用户的潜在需求，构建搜索引擎，广告投放系统来提升服务精准度，理解用户使用产品的心理动机和行为习惯，完善产品运营，提升服务质量，达到产品获客、黏客、留客目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果给你一堆用户数据，没有打标签。你该如何处理\n",
    "\n",
    "  1. 可以找一些专家对其中部分数据打标签，然后通过机器学习分类方法对剩余数据打标签打标签\n",
    "  2. 通过系统生成一批问卷调查，让用户对这批数据打标签\n",
    "  3. 通过聚类方法设定标签数量，来对数据自动打标签\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准确率和精确率有何不同（评估指标）\n",
    "   准确率是混合矩阵中对数据预测正确的概率，它是结果为1的正确预测概率和结果为0的正确预测概率之和，用于表示被分对的样本数除以所有的样本数\n",
    "   精确率是混合矩阵中对正向目标数据中预测正确的概率，它是结果为1的样本数占实际样本数量的概率 用于表示在被所有预测为正的样本中实际为正样本的概率"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如果你使用大众点评，想要给某个餐厅打标签。这时系统可以自动提示一些标签，你会如何设计（标签推荐）\n",
    "    当该餐厅是新餐厅的时候，可以给用户u推荐整个系统最热门的标签或者推荐他自己经常使用的标签\n",
    "    当用户是没用使用过标签的时候给用户u推荐物品i上最热门的标签\n",
    "    如果餐厅有标签，用户也打过标签，可以使用将餐厅标签与用户自己的标签融合加权方式，对最终权重大的标签推荐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T02:59:35.952307Z",
     "start_time": "2020-11-26T02:59:35.932416Z"
    }
   },
   "source": [
    "## 我们今天使用了10种方式来解MNIST，这些方法有何不同？你还有其他方法来解决MNIST识别问题么（分类方法）\n",
    "\n",
    "    Logistic Regression :逻辑回归模型，可解释性非常好\n",
    "    CART，ID3（决策树）：树模型，同增同减操作，所以不需要归一化和one-hot操作\n",
    "    朴素贝叶斯：假设各个特征彼此独立的贝叶斯概率\n",
    "    SVM：基于训练集合D DD在样本空间中找到一个划分超平面，将不同的类别样本分开\n",
    "    KNN：通过测量不同特征值之间的距离进行分类，如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，\n",
    "         则该样本也属于这个类别\n",
    "    Adaboost: 迭代算法，针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）\n",
    "    XGBoost: Boost算法一个成员根本思想在于通过多个简单的弱分类器通过不断加入特征新的树最快速度降低残差，构建出准确率很高的强分类器\n",
    "    TPOT：通过遗传算法进行特征选择和算法模型选择，自动完成机器学习流程设计中特征工程、模型选择、超参优化三个重要环节，\n",
    "          以此来生成机器学习的主线代码\n",
    "    keras：是神经网络模型算法，通过深度计算方式来实现\n",
    "\n",
    "    分类方法主要包括：随机森林 朴素贝叶斯  决策树 LDA等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对Delicious数据集，对SimpleTagBased算法进行改进（使用NormTagBased、TagBased-TFIDF算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:24:26.334587Z",
     "start_time": "2020-11-26T14:52:53.039093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据加载...\n",
      "数据集大小为 437593.\n",
      "设置tag的人数 1867.\n",
      "数据加载完成\n",
      "\n",
      "训练集样本数 1860, 测试集样本数 1793\n",
      "user_tags, tag_items, user_items初始化完成.\n",
      "user_tags大小 1860, tag_items大小 36884, user_items大小 1860\n",
      "recommend推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.829%      0.355%\n",
      " 10      0.633%      0.542%\n",
      " 20      0.512%      0.877%\n",
      " 40      0.381%      1.304%\n",
      " 60      0.318%      1.635%\n",
      " 80      0.276%      1.893%\n",
      "100      0.248%      2.124%\n",
      "recommend_normalTagBased推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      0.907%      0.388%\n",
      " 10      0.638%      0.546%\n",
      " 20      0.507%      0.868%\n",
      " 40      0.356%      1.218%\n",
      " 60      0.287%      1.476%\n",
      " 80      0.255%      1.750%\n",
      "100      0.241%      2.061%\n",
      "recommend_TagBasedTFIDF推荐结果评估\n",
      "  N        精确率        召回率\n",
      "  5      1.008%      0.431%\n",
      " 10      0.761%      0.652%\n",
      " 20      0.549%      0.940%\n",
      " 40      0.402%      1.376%\n",
      " 60      0.328%      1.687%\n",
      " 80      0.297%      2.033%\n",
      "100      0.269%      2.306%\n"
     ]
    }
   ],
   "source": [
    "# 使用SimpleTagBased算法对Delicious2K数据进行推荐\n",
    "# 原始数据集：https://grouplens.org/datasets/hetrec-2011/\n",
    "# 数据格式：userID     bookmarkID     tagID     timestamp\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"./code/delicious-2k/user_taggedbookmarks-timestamps.dat\"\n",
    "# 字典类型，保存了user对item的tag，即{userid: {item1:[tag1, tag2], ...}}\n",
    "records = {}\n",
    "# 训练集，测试集\n",
    "train_data = dict()\n",
    "test_data = dict()\n",
    "# 用户标签，商品标签\n",
    "user_tags = dict()\n",
    "tag_items = dict()\n",
    "user_items = dict()\n",
    "tag_users= dict()\n",
    "items_users= dict()\n",
    "\n",
    "# 使用测试集，计算准确率和召回率\n",
    "def precisionAndRecall(N,methods=\"recommend\"):\n",
    "    hit = 0\n",
    "    h_recall = 0\n",
    "    h_precision = 0\n",
    "    for user,items in test_data.items():\n",
    "        if user not in train_data:\n",
    "            continue\n",
    "        # 获取Top-N推荐列表\n",
    "        # 通过globals()返回的字典查找函数方法，来获取排名\n",
    "        rank = globals()[methods](user, N)\n",
    "\n",
    "#         rank = recommend(user, N)\n",
    "        for item,rui in rank:\n",
    "            if item in items:\n",
    "                hit = hit + 1\n",
    "        h_recall = h_recall + len(items)\n",
    "        h_precision = h_precision + N\n",
    "    #print('一共命中 %d 个, 一共推荐 %d 个, 用户设置tag总数 %d 个' %(hit, h_precision, h_recall))\n",
    "    # 返回准确率 和 召回率\n",
    "    return (hit/(h_precision*1.0)), (hit/(h_recall*1.0))\n",
    "\n",
    "# 对用户user推荐Top-N\n",
    "def recommend(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti\n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "#NormTagBased算法： score(u,i)= user_tags[u,t]/user_tags[u] * tag_items[t,i]/tag_items[t]\n",
    "# 对score进行归一化\n",
    "def recommend_normalTagBased(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            norm = len(user_tags[user].items())*len(tag_users[tag].items())\n",
    "#             norm = len(user_tags[user].items())*len(tag_itemsg[tag].items())\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti/norm\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti/norm\n",
    "            \n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "#NormTagBased算法： score(u,i)= user_tags[u,t]/log(user_tags[u]+1) * tag_items[t,i]\n",
    "# 对score进行归一化\n",
    "def recommend_TagBasedTFIDF(user, N):\n",
    "    recommend_items=dict()\n",
    "    # 对Item进行打分，分数为所有的（用户对某标签使用的次数 wut, 乘以 商品被打上相同标签的次数 wti）之和\n",
    "    tagged_items = user_items[user]     \n",
    "    for tag, wut in user_tags[user].items():\n",
    "        #print(self.user_tags[user].items())\n",
    "        for item, wti in tag_items[tag].items():\n",
    "            if item in tagged_items:\n",
    "                continue\n",
    "            norm = math.log(len(tag_users[tag].items())+1,2)\n",
    "            #print('wut = %s, wti = %s' %(wut, wti))\n",
    "            if item not in recommend_items:\n",
    "                recommend_items[item] = wut * wti/norm\n",
    "            else:\n",
    "                recommend_items[item] = recommend_items[item] + wut * wti/norm\n",
    "            \n",
    "    return sorted(recommend_items.items(), key=operator.itemgetter(1), reverse=True)[0:N]\n",
    "\n",
    "\n",
    "# 使用测试集，对推荐结果进行评估\n",
    "def testRecommend(methods=\"recommend\"):\n",
    "    print(\"{}推荐结果评估\".format(methods))\n",
    "    print(\"%3s %10s %10s\" % ('N',\"精确率\",'召回率'))\n",
    "    for n in [5,10,20,40,60,80,100]:\n",
    "        precision,recall = precisionAndRecall(n,methods)\n",
    "        print(\"%3d %10.3f%% %10.3f%%\" % (n, precision * 100, recall * 100))\n",
    "        \n",
    "# 数据加载\n",
    "def load_data():\n",
    "    print(\"开始数据加载...\")\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    for i in range(len(df)):\n",
    "        uid = df['userID'][i]\n",
    "        iid = df['bookmarkID'][i]\n",
    "        tag = df['tagID'][i]\n",
    "        # 键不存在时，设置默认值{}\n",
    "        records.setdefault(uid,{})\n",
    "        records[uid].setdefault(iid,[])\n",
    "        records[uid][iid].append(tag)\n",
    "    print(\"数据集大小为 %d.\" % (len(df)))\n",
    "    print(\"设置tag的人数 %d.\" % (len(records)))\n",
    "    print(\"数据加载完成\\n\")\n",
    "\n",
    "# 将数据集拆分为训练集和测试集\n",
    "def train_test_split(ratio, seed=100):\n",
    "    random.seed(seed)\n",
    "    for u in records.keys():\n",
    "        for i in records[u].keys():\n",
    "            # ratio比例设置为测试集\n",
    "            if random.random()<ratio:\n",
    "                test_data.setdefault(u,{})\n",
    "                test_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    test_data[u][i].append(t)\n",
    "            else:\n",
    "                train_data.setdefault(u,{})\n",
    "                train_data[u].setdefault(i,[])\n",
    "                for t in records[u][i]:\n",
    "                    train_data[u][i].append(t)        \n",
    "    print(\"训练集样本数 %d, 测试集样本数 %d\" % (len(train_data),len(test_data)))\n",
    "\n",
    "# 设置矩阵 mat[index, item] = 1\n",
    "def addValueToMat(mat, index, item, value=1):\n",
    "    if index not in mat:\n",
    "        mat.setdefault(index,{})\n",
    "        mat[index].setdefault(item,value)\n",
    "    else:\n",
    "        if item not in mat[index]:\n",
    "            mat[index][item] = value\n",
    "        else:\n",
    "            mat[index][item] += value\n",
    "\n",
    "\n",
    "# 使用训练集，初始化user_tags, tag_items, user_items\n",
    "def initStat():\n",
    "    records=train_data\n",
    "    for u,items in records.items():\n",
    "        for i,tags in items.items():\n",
    "            for tag in tags:\n",
    "                #print tag\n",
    "                # 用户和tag的关系\n",
    "                addValueToMat(user_tags, u, tag, 1)\n",
    "                # tag和item的关系\n",
    "                addValueToMat(tag_items, tag, i, 1)\n",
    "                # 用户和item的关系\n",
    "                addValueToMat(user_items, u, i, 1)\n",
    "                # tag和用户关系\n",
    "                addValueToMat(tag_users, tag, u, 1)\n",
    "                \n",
    "                # tag和用户关系\n",
    "                addValueToMat(tag_users, tag, u, 1)\n",
    "                \n",
    "                # item和用户关系\n",
    "                addValueToMat(items_users, i, u, 1)\n",
    "    print(\"user_tags, tag_items, user_items初始化完成.\")\n",
    "    print(\"user_tags大小 %d, tag_items大小 %d, user_items大小 %d\" % (len(user_tags), len(tag_items), len(user_items)))\n",
    "\n",
    "# 数据加载\n",
    "load_data()\n",
    "# 训练集，测试集拆分，20%测试集\n",
    "\n",
    "train_test_split(0.2)\n",
    "initStat()\n",
    "testRecommend()\n",
    "testRecommend('recommend_normalTagBased')\n",
    "testRecommend('recommend_TagBasedTFIDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对Titanic数据进行清洗，建模并对乘客生存进行预测。使用之前介绍过的10种模型中的至少2种（包括TPOT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T15:58:02.220228Z",
     "start_time": "2020-11-26T15:58:02.138983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看数据信息：列名、非空个数、类型等\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "------------------------------\n",
      "查看数据摘要\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "------------------------------\n",
      "查看离散数据分布\n",
      "                    Name   Sex  Ticket Cabin Embarked\n",
      "count                891   891     891   204      889\n",
      "unique               891     2     681   147        3\n",
      "top     Flynn, Mr. James  male  347082    G6        S\n",
      "freq                   1   577       7     4      644\n",
      "------------------------------\n",
      "查看前5条数据\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "------------------------------\n",
      "查看后5条数据\n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.0      0      0      112053  30.00   B42        S  \n",
      "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.0      0      0      111369  30.00  C148        C  \n",
      "890    male  32.0      0      0      370376   7.75   NaN        Q  \n",
      "S    644\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      "特征值\n",
      "     PassengerId  Pclass                                          Name  \\\n",
      "0            892       3                              Kelly, Mr. James   \n",
      "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                     Myles, Mr. Thomas Francis   \n",
      "3            895       3                              Wirz, Mr. Albert   \n",
      "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "..           ...     ...                                           ...   \n",
      "413         1305       3                            Spector, Mr. Woolf   \n",
      "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                           Ware, Mr. Frederick   \n",
      "417         1309       3                      Peter, Master. Michael J   \n",
      "\n",
      "        Sex       Age  SibSp  Parch              Ticket      Fare Cabin  \\\n",
      "0      male  34.50000      0      0              330911    7.8292   NaN   \n",
      "1    female  47.00000      1      0              363272    7.0000   NaN   \n",
      "2      male  62.00000      0      0              240276    9.6875   NaN   \n",
      "3      male  27.00000      0      0              315154    8.6625   NaN   \n",
      "4    female  22.00000      1      1             3101298   12.2875   NaN   \n",
      "..      ...       ...    ...    ...                 ...       ...   ...   \n",
      "413    male  30.27259      0      0           A.5. 3236    8.0500   NaN   \n",
      "414  female  39.00000      0      0            PC 17758  108.9000  C105   \n",
      "415    male  38.50000      0      0  SOTON/O.Q. 3101262    7.2500   NaN   \n",
      "416    male  30.27259      0      0              359309    8.0500   NaN   \n",
      "417    male  30.27259      1      1                2668   22.3583   NaN   \n",
      "\n",
      "    Embarked  \n",
      "0          Q  \n",
      "1          S  \n",
      "2          Q  \n",
      "3          S  \n",
      "4          S  \n",
      "..       ...  \n",
      "413        S  \n",
      "414        C  \n",
      "415        S  \n",
      "416        S  \n",
      "417        C  \n",
      "\n",
      "[418 rows x 11 columns]\n",
      "['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hui/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:1485: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# 数据加载\n",
    "train_data = pd.read_csv('./code/titanic/train.csv')\n",
    "test_data = pd.read_csv('./code/titanic/test.csv')\n",
    "# 数据探索\n",
    "# 查看train_data信息\n",
    "#pd.set_option('display.max_columns', None) #显示所有列\n",
    "print('查看数据信息：列名、非空个数、类型等')\n",
    "print(train_data.info())\n",
    "print('-'*30)\n",
    "print('查看数据摘要')\n",
    "print(train_data.describe())\n",
    "print('-'*30)\n",
    "print('查看离散数据分布')\n",
    "print(train_data.describe(include=['O']))\n",
    "print('-'*30)\n",
    "print('查看前5条数据')\n",
    "print(train_data.head())\n",
    "print('-'*30)\n",
    "print('查看后5条数据')\n",
    "print(train_data.tail())\n",
    "\n",
    "# 使用平均年龄来填充年龄中的nan值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)\n",
    "# 使用票价的均值填充票价中的nan值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)\n",
    "\n",
    "print(train_data['Embarked'].value_counts())\n",
    "# 使用登录最多的港口来填充登录港口的nan值\n",
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)\n",
    "# 特征选择\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "# test_labels = test_data['Survived']\n",
    "print('特征值')\n",
    "print(test_data)\n",
    "\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(train_features, test_features)\n",
    "print(tpot.score(train_labels, y_test))\n",
    "tpot.export('tpot_iris_pipeline.py')\n",
    "\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "dvec=DictVectorizer(sparse=False)\n",
    "train_features=dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)\n",
    "# # 构造ID3决策树\n",
    "# clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# # 决策树训练\n",
    "# clf.fit(train_features, train_labels)\n",
    "\n",
    "# test_features=dvec.transform(test_features.to_dict(orient='record'))\n",
    "# # 决策树预测\n",
    "# pred_labels = clf.predict(test_features)\n",
    "\n",
    "# # 得到决策树准确率(基于训练集)\n",
    "# acc_decision_tree = round(clf.score(train_features, train_labels), 6)\n",
    "# print(u'score准确率为 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# # 使用K折交叉验证 统计决策树准确率\n",
    "# print(u'cross_val_score准确率为 %.4lf' % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
